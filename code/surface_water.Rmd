---
title: "Surface Water"
output:
  pdf_document: default
  html_document: default
  toc: yes
  toc_depth: 3
---

# The Prompt 

Background: The attached Excel file (Dataset_C) contains analytical chemistry data from a variety of surface water stations. 
 
Given the attached dataset:

* Develop a tool for plotting time series by station and analyte.

* Symbolize data by season.

* Add a horizontal line for the mean concentration to each plot.

* Which analytes have the most seasonal variability?  What tools/approach did you use?

* Produce a pdf of your plots.

# The Answer/Analysis

There seem to be two stages of tasks set forth here: the first, to do some data visualization of a multi-dimensional data set, and the second, to determine which of the analytes show strong seasonal variability. Before diving into the seasonality analysis, I first tackled the data visualization of the observations by analyte and station. 

##  First Forays and Data Explorations

Reading in the data, loading relevant libraries:
```{r, setup, include=TRUE, warning=F, message=F}
rm(list=ls()) # Clear working environment

library("MapSuite") #Self-written library, has many common libs as dependencies
library("hexbin") # For the hex-bin plots

# Read in surface water observations as data.table sw
#setwd("/Users/stubbsrw/Documents/git_code/stubbs_repo/fe_problems/code/")
sw<-fread("Dataset_C.csv")
```

Next, I calculate various columns describing the date of observations that may be useful later on.
```{r, include=TRUE, warning=F, message=F}
# Parse out date information from character date column
  sw[,index:=seq(1:nrow(sw))]
  sw[,Month:=as.numeric(strsplit(SampleDate,"/")[[1]][1]),by=index] 
  sw[,Day:=as.numeric(strsplit(SampleDate,"/")[[1]][2]),by=index]
  sw[,Year:=as.numeric(strsplit(SampleDate,"/")[[1]][3]),by=index]
# Create formal column of integer-date
  sw[,Date:=as.IDate(paste0((2000+Year),"-",Month,"-",Day))] 
# For each analyte, discover the minimum date; generate a yr/month index from that date:
  sw[,year_index:=Year-min(Year,na.rm=T),by=StandardAnalyte]
# Number of months from the start of the samples
  sw[,month_index:=12*(Year-min(Year,na.rm=T)) + Month] 
```

In order to symbolize by season, I am lumping observations into seasons by month. It's certainly possible to do this a more sophisticated way, based on cut points of the equinoxes; for now, however, month-by-month approximations seem adequate.

```{r, warning=F, message=F}
sw[Month %in% c(12,1,2), Season:='Winter']
sw[Month %in% c(3,4,5), Season:='Spring']
sw[Month %in% c(6,7,8), Season:='Summer']
sw[Month %in% c(9,10,11), Season:='Fall']
# Defining Season as a factor variable
sw[,Season:=factor(Season, levels = c("Winter","Spring","Summer","Fall"))] 
```

Also, I calculate the mean concentration of each analyte, as well as a log-transformed version of the observations, for convenience later on. 

```{r, include=TRUE, warning=F, message=F}
# Add variables on mean concentration for each 
#analyte by site and globally/for all samples
sw[,mean_conc:=mean(StandardResult,na.rm=T),
     by=list(StationName,StandardAnalyte)]

# Just in case a log-transform would be more informative, 
# although NaNs will exist where the observation is negative
sw[,log_obs:=log(StandardResult)] 
```

## Plotting Each Analyte by Station, Over Time

In order to make plotting by each station and analyte straightforward, I have written a function that takes a station name, and analyte name as input, and returns a plot of the points, color-coded by season, with a black line representing the mean of the analyte at that station across the full time period. 

```{r}
# Define a color pallette for the factor variable, Season
SeasonColors<-wpal("foliage",noblack=T,n=4)
names(SeasonColors) <- c("Winter","Spring","Summer","Fall")

# Define function to generate plot
  MakeAnalyteTSPlot<-function(a,s){
  
    p<-ggplot(sw[StandardAnalyte==a & StationName==s], 
      aes(x= Date, y=StandardResult, color=Season)) + geom_point(size=4) + 
      xlab("Date of Sample") + 
      scale_x_date(labels = function(x) format(x, "%b-%y")) + 
      ylab(sw[StandardAnalyte==a & StationName==s]$StandardUnit[1]) + 
      ggtitle(paste0(a), subtitle=paste0("Station ",s)) + theme_bw()  + 
      scale_colour_manual(name = "Seasons",values = SeasonColors, drop=F) +
      geom_hline(yintercept = sw[StandardAnalyte==a &
      StationName==s]$mean_conc[1]) + 
      annotate("text", min(sw[StandardAnalyte==a & StationName==s]$Date), 
               sw[StandardAnalyte==a & StationName==s]$mean_conc[1], 
               vjust = -1, label = "Mean")
  
  return(p)  
  }
```

Before iterating over all of the analytes and stations, and saving them to PDFs, I will test out the plotting function on one analyte, and one station:

```{r}
# Make and print plot
  ts<-MakeAnalyteTSPlot(a="Temperature",s="FE-4023")
  print(ts)
```

Rather than simply having .PDFs, I sometimes find it useful to create a quick and dirty interactive visualization, with options to select and sub-set data. This won't work in a static file format like .PDF, but you can check out an interactive version of this plot, for each station and analyte, at this URL:  **!!!<include screenshot here>!!!**

\newpage

# Quantifying Seasonality

The fundamental question here seems to be, "to what extent is the variability in the data due to seasonal effects, rather than annual or other differences"? Unfortunately, there aren't very many data points per station within this data set, and the time period of observation only lasts a few years' time. This makes disentangling variation in observations for each site difficult--the change in observed data could be due to measurement error, a product of seasonal variation, inter-annual variation, random noise, or the effect of a recent event, or the influence of other unknown factors.  

Without knowing the spatial location of any of the stations, I assumed that the stations would be nearby one another, and subject to (at least roughly) the same weather and insolation patterns--- if these stations were far apart, it would be even more difficult to measure the "seasonal component" of each analyte, since the magnitude of seasonal changes could be subject to variables such as latitude and elevation. 

Given the data constraints, a few different strategies came to mind. As a test case for each of them, I used temperature, since the right strategy would presumably show a seasonal effect for this analyte, and I had a sense of what probably "should" be happening (it presumably will get warmer in the summer months)!

As a first pass, I made a frequency plot that showed the observed values across the entire time period, for all stations. 

```{r, fig.asp=.5}
a<-"Temperature"

# Plotting using the HexBin Frequency graphics, where the number of 
#stations with an observation in that category is essentially heat-mapped
ggplot(sw[StandardAnalyte==a], aes(x=Date, y=StandardResult)) + 
  geom_hex() + # honeycomb-plot geometry
  scale_x_date(labels = function(x) format(x, "%b-%y")) + xlab("Time") +
  ylab(sw[StandardAnalyte==a]$StandardUnit[1]) + 
  ggtitle(paste0(a), 
          (subtitle="Observations Over Full Time Series, All Stations")) + 
  theme_bw() + scale_fill_gradientn(colors=wpal("berries")) +
  guides(fill=guide_colourbar(title="N Stations", 
                              title.position="top", barheight=10, barwidth=1,
                              label=TRUE, ticks=FALSE, direction="vertical")) 
```

Let's try pooling observations across stations and also years, to get a rough sense of seasonality from a graphical persepective: 

```{r,fig.asp=.5}
# Plotting by Month, for all years, all stations 
months_in_order<-c("Jan","Feb","Mar","Apr",
                   "May","Jun","Jul","Aug",
                   "Sep","Oct","Nov","Dec")

# Non-Transformed
ggplot(sw[StandardAnalyte==a], aes(x= Month, y=log_obs)) + geom_hex() + 
  ggtitle(paste0(a), 
          subtitle="Observations in Each Month, All Years, All Stations") + 
  scale_x_continuous(limits=c(1,12),breaks=seq(1,12), labels=months_in_order) +
  ylab(sw[StandardAnalyte==a]$StandardUnit[1]) + theme_bw() + 
    scale_fill_gradientn(colors=wpal("berries")) +
  guides(fill=guide_colourbar(title="N Stations", title.position="top", 
                              barheight=10, barwidth=1, direction="vertical",
                              label=TRUE, ticks=FALSE)) 
```

Looks like there's a trend there to the casual eye, but interesting to note that there are no observations during the winter months whatsoever. 


## Generate PDF of results for each Analyte

```{r, eval=F}
for ( a in unique(sw$StandardAnalyte)){
  analyte<-sw[StandardAnalyte==a] # Subset data to only relevant analyte
  
  # Start a PDF document of results
    pdf(paste0("/Users/stubbsrw/Documents/git_code/stubbs_repo/fe_problems/results/surface_water/",a,".pdf"))

     # Generate plot for each Analyte over time, for each station. 
      for (s in unique(sw$StationName)){ 
        ts<-MakeAnalyteTSPlot(a=a,s=s) # Get inputs from UI, use function
        print(ts)  # Print it to the PDF
      }
    
  dev.off() # Close PDF for each 
}
```


## Use Linear Regression Model Coefficients to quantify seasonal component, by season

Strategy number 1 is running a regression with terms for each year and for each season, to get a sense of the significance and magnitude of seasonal coefficients while "controlling" for annual differences. Both of these variables were included in a linear regression as "dummy" variables, where categories for year and season were represented numerically as presence-absence for each category. Including the season and year variables in the model this way removes the idea of temporal "trajectory" or sequence, which may not exist, or which may reverse direction mid-way in the time period due to some sort of effect or intervention. 

An initial stab at this model also included the sampling station as a variable as well-- however, insufficient data for many of the observation sites (on the order of 2-3 observations for the full time period for Temperature, for instance) makes inference about the individual station's expected deviation from the mean dubious, and virtually none of the sites were statistically significant in the model. Knowing that certain sites group together might be one way to improve this-- for instance, if all of the "FE-ET" stations could be considered together, it would likely improve the model's estimate, and would better disentangle variation due to site-specific conditions rather than seasonality or annual differences.

```{r}
# Testing out a linear model with temperature
mod <- lm(StandardResult  ~ as.factor(Year)+Season,data=analyte)
summary(mod)
```


```{r}
linear_model_results<-list() # Create empty list for results to go into 
analytes<-unique(sw$StandardAnalyte)
analytes<-analytes[!analytes %in% c("Alkalinity, Total", "DOC", "TOC", "TDS, ratio")] # Each of these are creating problems, so to save time, I'm just going to exclude them from this analysis

for (a in analytes){
  print(a)
  analyte<-sw[StandardAnalyte==a]
  
  # Run the model for the analyte
  mod <- lm(StandardResult  ~ as.factor(Year)+Season,data=analyte)

# Capture the regression coefficients produced by the model
  analyte_results<-data.table(analyte=a,coef=names(mod$coefficients),est=mod$coefficients,std_error=coef(summary(mod))[, "Std. Error"])

linear_model_results[[a]]<-analyte_results # Add the table of regression coefficients to the list
}

linear_model_results<-rbindlist(linear_model_results)

```

To get a sense of how substantial each season's effect is, we can compare the magnitude of the seasonal effect to the intercept (the otherwise expected value, once year is controlled for):

```{r}
linear_model_results[coef=="(Intercept)", intercept:=est]
linear_model_results[,intercept:=min(intercept,na.rm=T),by=analyte]
```

There is some discussion of the number of knots that are appropriate for seasonal land (not water) surface temperature changes in this paper here: http://www.mdpi.com/2072-4292/9/12/1254/pdf; however, it is discussed that the ideal number of knots for their model to represent seasonality (8) may not be appropriate for other applications. 

## Fit GAMM with knots for each season and year

Another possible approach is to fit a general additive mixed model, where the long-term time trend and the short-term, periodic seasonal trends are conceptualized by cubic splines. 

```{r}
# Pulling from https://www.fromthebottomoftheheap.net/2014/05/09/modelling-seasonal-data-with-gam/
# 1 knot every 6 months, 1 knot for each year of data?
m0 <- mgcv::gamm(StandardResult ~ s(Month, bs = "cc", k = 6) + s(month_index, k = 3) + as.factor(StationName), data = analyte)

plot(m0$gam, scale = 0, main = "GAMM fit", xlab = "Month", ylab = paste0(a," Seasonal Effect"))
```

Let's try a linear model with a sin and cosine function

```{r}
mod <- lm(log_obs  ~ sin(2*pi*(month_index/12))+cos(2*pi*(month_index/12))+StationName,data=analyte)
summary(mod)

plot(log_obs~month_index,data=analyte)
plot(analyte$month_index,mod$fitted.values)
```

## Linear Model with 

```{r}
mod <- lm(StandardResult  ~ sin(2*pi*(month_index/12))+cos(2*pi*(month_index/12))+StationName+Year,data=analyte)
summary(mod)

plot(StandardResult~month_index,data=analyte)
lines(mod$fitted.values,analyte$month_index,col=2)

analyte[,fitted:=mod$fitted.values]
```





```{r, echo = FALSE}
library(shiny) #Load shiny (an R package for interactive visuaizations)library:

# Initiate a shiny application
shiny::shinyApp(
  
  # Define the User Interface
    ui = pageWithSidebar(
      # Application title
        headerPanel("Time Series for Analyte-Station"),
    # Sidebar with a selectable input for analytes and stations
      sidebarPanel(
         selectInput("analyte", "Analyte:", 
                    choices = unique(sw$StandardAnalyte)),
          selectInput("station", "Station:", 
                    choices = unique(sw$StationName))
      ),
    # Show the time series GGplot in the main panel
      mainPanel(
        plotOutput("Analyte_Station_TimeSeries")
      )
    ),

  # Define the server/output
  server = function(input, output) {
    # Define the Analyte_Station_TimeSeries output  as the TS plot in fn above  
      output$Analyte_Station_TimeSeries <- renderPlot({
        ts<-MakeAnalyteTSPlot(a=input$analyte,s=input$station) # Get inputs from UI, use function
        print(ts)
      })
  },
  options = list(height = 500) # Define how big this visualization GUI is
)
```



